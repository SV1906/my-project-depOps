# -*- coding: utf-8 -*-
"""part2_Double_Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EjpOuEV0X_nYAXFuEbaAgl2kSZflu82g

# CS 5814 Homework 4, Part 2: Deep Q-Learning
"""

#Run 1
!pip3 install gym pyvirtualdisplay # gym is the OpenAI gym: https://github.com/openai/gym
!pip3 install xvfbwrapper pyvirtualdisplay
!pip3 install pyopengl
!pip3 install ffmpeg-python

#Run 2
!pip show gym

from google.colab import drive
drive.mount('/content/drive')

import os
datadir = "/content/drive/My Drive/johnsnow/part2/part2/" # path to the homework
if not os.path.exists(datadir):
  !ln -s "" $datadir # path to the homework
os.chdir(datadir)
!pwd

# !pip3 install --upgrade setuptools --user
# !pip3 install ez_setup
# !pip3 install gym[atari] # Atari games
# !pip3 install gym[accept-rom-license] # accept the license agreement

from skimage.color import rgb2gray



"""You should take a look at [this paper](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) before beginning. You'll be playing the game of __Breakout__."""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# !pip3 install model
import sys
import gym
import torch
import pylab
import random
import numpy as np
from collections import deque
from datetime import datetime
from copy import deepcopy
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
from model import Model
from utils import count_max_lives, check_if_live, process_frame, get_initialization_state
from config import *
from memory import ExperienceBuffer

import matplotlib.pyplot as plt
# %load_ext autoreload
# %autoreload 2

"""## Initialize env"""



"""Refer to the Gym [documentation](https://www.gymlibrary.dev/environments/atari/breakout/) for more details on the environment. There are three action_to_takes in this game - "left", "right" (to move the paddle) and "fire" (this releases the ball)."""

env = gym.make('BreakoutDeterministic-v4')
# v0 vs v4: v0 has repeat_action_to_take_probability of 0.25 (meaning 25% of the time the previous action_to_take will be used instead of the new action_to_take), while v4 has 0 (always follow your issued action_to_take)
# Deterministic: a fixed frame_counterskip of 4, while for the env without Deterministic, frame_counterskip is sampled from (2,5)
# https://github.com/openai/gym/issues/1280
state = env.reset()

!pip show gym



!pip install -U ale-py==0.8

# worked
!pip install gym[atari,accept-rom-license]==0.21.0

!pip install setuptools==65.5.0 pip==21  # gym 0.21 installation is broken with more recent versions

!pip install gym==0.18.0

max_number_of_lives_in_game = count_max_lives(env)
state_size = env.observation_space.shape
action_to_take_size = 3

"""## Create your agent

The agent is defined in the __your_agent.py__ file. We have coded a network for you already in the __model.py__ file. You shouldn't change the implementation we have there for fairness.

Once you get that working, you'll need to then create a double agent (see [this paper](https://arxiv.org/pdf/1509.06461.pdf)) in the __your_double_agent.py__ file. We have a switch below which determines which to train.
"""

double_d = True # switch
learning_rate = 0.005
if double_d:
    from your_double_agent import EnvironmentAgent
else:
    from your_agent import EnvironmentAgent

agent = EnvironmentAgent(action_to_take_size) # buff size
rl_reward_from_eval_window = 100  # or any other suitable value
rl_reward_from_eval = deque(maxlen=rl_reward_from_eval_window) # This is avg rl_reward from 100 games
cnt_frame= 0
memory_size = 0

"""### Training"""

num_episodes = 5000

rl_rewards, episodes = [], []
best_rl_reward_from_eval = 0
for e in range(num_episodes):
    done = False
    run_score = 0

    state_history = np.zeros([5, 84, 84], dtype=np.uint8) # 5 x 84 x 84
    step = 0
    state = env.reset() # reset the environment for new episode
    subsequent_state = state
    life = max_number_of_lives_in_game # resetting life count - new game

    get_initialization_state(state_history, state, history_length) # set up the states

    while not done:
        step += 1
        cnt_frame+= 1

        if step > 1 and len(np.unique(subsequent_state[:189] == state[:189])) < 2:
            action = 0  # This is going to "fire"  - this is checking to see if you need to start the game
        else:
            action = agent.select_action(np.float32(state_history[:4, :, :]) / 255.) # converting imgs to 0-1
        state = subsequent_state
        subsequent_state, rl_reward, done, info_dictionary = env.step(action + 1)
        # done = exter or truncated

        frame_counter_subsequent_state = process_frame(subsequent_state)
        state_history[4, :, :] = frame_counter_subsequent_state
        last_state = check_if_live(life, info_dictionary['lives'])

        life = info_dictionary['lives']
        r = rl_reward

        agent.sys_memory.record(deepcopy(frame_counter_subsequent_state), action, r, done)

        # only train when ready
        if(cnt_frame>= training_frames):
            agent.p_net_training(cnt_frame)
            if double_d and (cnt_frame% target_update_frequency)== 0:
                agent.target_to_policy()
        run_score += rl_reward
        state_history[:4, :, :] = state_history[1:, :, :] # cut off last

        if done:
            rl_reward_from_eval.append(run_score)
            rl_rewards.append(np.mean(rl_reward_from_eval))
            episodes.append(e)
            pylab.plot(episodes, rl_rewards, 'b')
            pylab.xlabel('Number of Episodes')
            pylab.ylabel('Amount of Rewards')
            pylab.title('Plot of Episodes and Rewards')
            pylab.savefig("./deep_q.png")

            print("For episode:", e, "  the run_score was:", run_score, "  and mem length:",
                  len(agent.sys_memory), "  eps:", agent.eps, "   steps:", step,
                  "   lr:", agent.optimizer.param_groups[0]['lr'], "    eval rl_reward:", np.mean(rl_reward_from_eval))

            ### You can change this to whatever you want
            if np.mean(rl_reward_from_eval) > 5 and np.mean(rl_reward_from_eval) > best_rl_reward_from_eval:
                torch.save(agent.policy_network, "./dqn.pth")
                best_rl_reward_from_eval = np.mean(rl_reward_from_eval)

"""## Visualizing the Game

Be careful - you don't want to run this twice in the same kernel or it will crash. Recommend you save your model before making visualization.
"""

torch.save(agent.policy_network, "/content/drive/MyDrive/part2-Sandhya/part2/dqn_last.pth")



# from gym.wrappers import Monitor # `from gym.wrappers import RecordVideo` is another option
from gym.wrappers import RecordVideo
from IPython.display import HTML
from IPython import display as ipythondisplay
import glob
import io
import base64
from pyvirtualdisplay import Display

def vis_curr(env, step=0, info_dictionary=""):
    plt.figure(3)
    plt.clf()
    plt.imshow(env.render(mode='rgb_array'))
    plt.title("%s | Curr Step: %d %s" % ("Game",step, info_dictionary))
    plt.axis('off')

    ipythondisplay.clear_output(wait=True) # for jupyter notebook
    ipythondisplay.display(plt.gcf()) # for jupyter

# Making a video of the jupyter
def make_video_of_jupyter():
    videos_from_glob = glob.glob('video/*.mp4')
    if len(videos_from_glob) > 0:
        mp4 = videos_from_glob[0] # video path
        video = io.open(mp4, 'r+b').read() # load in the video
        encoded = base64.b64encode(video) # encode in base 64  # this code below creates html for the video in jupyter
        ipythondisplay.display(HTML(data='''<video alt="test" autoplay
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
    else:
        print("No video")


def environment_writer(env):
    env = RecordVideo(env, './video')
    return env

display = Display(visible=0, size=(300, 200))
display.start()

# Load agent
# might need to write code to load the agent if you are resuming here:
#load here
#############
# agent.eps = 0.0 # Why would we want this?
agent = EnvironmentAgent(action_to_take_size)
agent.load_policy_network("/content/drive/MyDrive/part2-Sandhya/part2/dqn_last.pth")
agent.eps = 0.0 # Why would we want this?

env = gym.make('BreakoutDeterministic-v4',render_mode='rgb_array')
env = environment_writer(env)

done = False
run_score = 0
step = 0
state = env.reset()
subsequent_state = state
life = max_number_of_lives_in_game # this stuff is all same as above
state_history = np.zeros([5, 84, 84], dtype=np.uint8)
# get_initialization_state(state_history, state)

while not done:

    # env.render()
#     vis_curr(env,step)

    step += 1
    cnt_frame+= 1

    if step > 1 and len(np.unique(subsequent_state[:189] == state[:189])) < 2:
        action = 0  # This is going to "fire"  - this is checking to see if you need to start the game
    else:
        action = agent.select_action(np.float32(state_history[:4, :, :]) / 255.) # converting imgs to 0-1
    state = subsequent_state

    # subsequent_state, rl_reward, done, info_dictionary = env.step(action_to_take + 1)
    subsequent_state, rl_reward, done, info_dictionary = env.step(action + 1)

    frame_counter_subsequent_state = process_frame(subsequent_state)
    state_history[4, :, :] = frame_counter_subsequent_state
    last_state = check_if_live(life, info_dictionary['lives'])

    life = info_dictionary['lives'] # lives in gym
    r = np.clip(rl_reward, -1, 1)  # clipping reward between -1 and 1
    r = rl_reward

    agent.sys_memory.record(deepcopy(frame_counter_subsequent_state), action, r, last_state)

    run_score += rl_reward

    state_history[:4, :, :] = state_history[1:, :, :]

env.close()
make_video_of_jupyter()
display.stop()

# !pip install xvfbwrapper
!sudo apt-get install xvfb

"""## What to include:
Your submission should include videos of your run, along with results from both the DQN and double DQN network showing training progress and final scores.
You should run both the same number of iterations and below write a **brief** analysis of any findings of your results.
"""

